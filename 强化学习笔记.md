### 基础概念

       **状态**是对世界的完整描述，不会隐藏世界的信息。**观测**是对状态的部分描述，可能会遗漏一些信息。

        当智能体的状态与环境的状态等价的时候，即当智能体能够观察到环境的所有状态时，我们称这个环 境是完全可观测的（fully observed）。在这种情况下面，强化学习通常被建模成一个**马尔可夫决策过程MDP**的问题。

        部分可观测马尔可夫决策过程（partially observable Markov decision process, POMDP）。部分可观测马尔可夫决策过程可以用一个七元组描述：(S,A,T,R,Ω,O,γ)。其中 S 表示状态空间，为隐变量，A 为动作空间，T(s′∣s,a) 为状态转移概率，R 为奖励函数，Ω(o∣s,a) 为观测概率，O 为观测空间，γ 为折扣系数。

$$
policy:\pi(s)
\\
value function: v(s)
$$

价值函数：

$$
V_\pi(s)=E_\pi(G_t|S_t=s)=E_\pi\left[\sum_{k=0}^\infin 
\gamma^kR_{t+k+1}|S_t=s\right].\forall s\in S
$$

动作价值函数：

$$
Q_\pi(s,a)=E_\pi\left[G_t|S_t=s,A_t=a\right]
=E_\pi\left[\sum_{k=0}^\infin
\gamma^kR_{t+k+1}|S_t=s,A_t=a\right]
$$

**环境相关：**

状态转移概率：

$$
p_{ss'}^a=p(S_{t+1}=s'|S_t=s,A_t=a)
$$

奖励函数

$$
R(s,a) = E\left[
R_{t+1}|S_t=s,A_t=a
\right]
$$

**分类：**

1. 基于价值的智能体（value-based agent）:Q学习（Q-learning）、 Sarsa

2. 基于策略的智能体（policy-based agent）:策略梯度（Policy Gradient，PG）算法

3. 演员-评论员智能体（actor-critic agent）

基于价值迭代的方法只能应用在不连续的、离散的环境下

1. 有模型（model-based）强化学习智能体

2. *免模型（model-free）强化学习智能体*：直接与真实环境进行交互

K-臂赌博机与UCB算法 [UCB算法简介及其bound证明 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/394688802)
